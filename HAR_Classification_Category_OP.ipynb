{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----------------------------------Human Activity Recorder------without Dimensionality redcution---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karthikeyan\\Anaconda3\\anoconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time-logistic regression:  3.3477933406829834\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.88      0.89       243\n",
      "           2       0.80      0.82      0.81       229\n",
      "           3       0.84      0.79      0.82       239\n",
      "           4       0.79      0.80      0.79       289\n",
      "           5       0.76      0.81      0.78       254\n",
      "           6       0.96      0.94      0.95       238\n",
      "\n",
      "    accuracy                           0.84      1492\n",
      "   macro avg       0.84      0.84      0.84      1492\n",
      "weighted avg       0.84      0.84      0.84      1492\n",
      "\n",
      "Execution time-bagging classifier:  353.6915285587311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.88      0.86       243\n",
      "           2       0.88      0.78      0.83       229\n",
      "           3       0.84      0.85      0.85       239\n",
      "           4       0.96      0.77      0.85       289\n",
      "           5       0.77      0.98      0.86       254\n",
      "           6       0.99      0.97      0.98       238\n",
      "\n",
      "    accuracy                           0.87      1492\n",
      "   macro avg       0.88      0.87      0.87      1492\n",
      "weighted avg       0.88      0.87      0.87      1492\n",
      "\n",
      "Execution time-Random forest:  1.5198767185211182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.85      0.83       243\n",
      "           2       0.79      0.80      0.79       229\n",
      "           3       0.89      0.80      0.84       239\n",
      "           4       0.79      0.83      0.81       289\n",
      "           5       0.82      0.80      0.81       254\n",
      "           6       0.98      0.97      0.98       238\n",
      "\n",
      "    accuracy                           0.84      1492\n",
      "   macro avg       0.85      0.84      0.84      1492\n",
      "weighted avg       0.84      0.84      0.84      1492\n",
      "\n",
      "Execution time-decision tree:  2.304438591003418\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.77      0.75       243\n",
      "           2       0.78      0.52      0.62       229\n",
      "           3       0.60      0.74      0.66       239\n",
      "           4       0.93      0.68      0.79       289\n",
      "           5       0.70      0.96      0.81       254\n",
      "           6       0.94      0.92      0.93       238\n",
      "\n",
      "    accuracy                           0.76      1492\n",
      "   macro avg       0.78      0.76      0.76      1492\n",
      "weighted avg       0.79      0.76      0.76      1492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "df = pd.read_csv(\"HAR.csv\", header=None)\n",
    "#print(df)\n",
    "\n",
    "#statistic report\n",
    "#print(df.describe())\n",
    "\n",
    "#training and testing\n",
    "X_test = df.drop(df.columns[[561]],axis=1)\n",
    "y_test= df[561]\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "\n",
    "df1=pd.read_csv(\"HAR_train.csv\",header=None)\n",
    "X_train = df1.drop(df1.columns[[561]],axis=1)\n",
    "y_train= df1[561]\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "#using Logistic \n",
    "start=time.time()\n",
    "model_logistic = LogisticRegression(random_state=10)\n",
    "model_logistic.fit(X_train,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-logistic regression: \",end - start)\n",
    "y_pred = model_logistic.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "#using algorithms-bagging classifiers\n",
    "start=time.time()\n",
    "model = BaggingClassifier(n_estimators=100, random_state=0)\n",
    "model.fit(X_train,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-bagging classifier: \",end - start)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "#uisng Random forest\n",
    "\n",
    "start=time.time()\n",
    "model1 = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "model1.fit(X_train,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-Random forest: \",end - start)\n",
    "y_pred = model1.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "#using Decision tree\n",
    "start=time.time()\n",
    "model2 = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "model2.fit(X_train,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-decision tree: \",end - start)\n",
    "y_pred = model2.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------with Dimensionality reduction----------------------PCA------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for transforming :  0.39577388763427734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karthikeyan\\Anaconda3\\anoconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time-logistic regression:  0.5681862831115723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.67      0.52       243\n",
      "           2       0.52      0.41      0.46       229\n",
      "           3       0.45      0.19      0.27       239\n",
      "           4       0.39      0.87      0.54       289\n",
      "           5       0.12      0.04      0.06       254\n",
      "           6       0.10      0.04      0.05       238\n",
      "\n",
      "    accuracy                           0.38      1492\n",
      "   macro avg       0.33      0.37      0.32      1492\n",
      "weighted avg       0.33      0.38      0.32      1492\n",
      "\n",
      "[[162  48  33   0   0   0]\n",
      " [ 62  94   9   9  51   4]\n",
      " [159  23  45   3   6   3]\n",
      " [  0   3   5 250   6  25]\n",
      " [  0  12   6 174  11  51]\n",
      " [  0   1   2 208  18   9]]\n",
      "Execution time-bagging classifier:  4.002925634384155\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.66      0.60       243\n",
      "           2       0.64      0.65      0.65       229\n",
      "           3       0.50      0.35      0.41       239\n",
      "           4       0.38      0.80      0.51       289\n",
      "           5       0.52      0.12      0.19       254\n",
      "           6       0.04      0.02      0.03       238\n",
      "\n",
      "    accuracy                           0.44      1492\n",
      "   macro avg       0.44      0.43      0.40      1492\n",
      "weighted avg       0.43      0.44      0.40      1492\n",
      "\n",
      "[[161  34  48   0   0   0]\n",
      " [ 34 149  28   9   1   8]\n",
      " [103  42  84   8   0   2]\n",
      " [  0   1   1 231  19  37]\n",
      " [  0   3   3 145  30  73]\n",
      " [  0   3   5 217   8   5]]\n",
      "Execution time-Random forest:  0.22087383270263672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.64      0.53       243\n",
      "           2       0.56      0.57      0.56       229\n",
      "           3       0.51      0.31      0.38       239\n",
      "           4       0.39      0.85      0.53       289\n",
      "           5       0.43      0.11      0.18       254\n",
      "           6       0.07      0.02      0.03       238\n",
      "\n",
      "    accuracy                           0.43      1492\n",
      "   macro avg       0.40      0.42      0.37      1492\n",
      "weighted avg       0.40      0.43      0.37      1492\n",
      "\n",
      "[[155  49  39   0   0   0]\n",
      " [ 71 131  16   8   0   3]\n",
      " [106  53  73   6   0   1]\n",
      " [  6   0   3 245  13  22]\n",
      " [  7   1   5 174  29  38]\n",
      " [  0   1   7 199  26   5]]\n",
      "Execution time-decision tree:  0.02498459815979004\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.57      0.65       243\n",
      "           2       0.68      0.50      0.58       229\n",
      "           3       0.51      0.76      0.61       239\n",
      "           4       0.40      0.81      0.54       289\n",
      "           5       0.56      0.14      0.23       254\n",
      "           6       0.01      0.00      0.01       238\n",
      "\n",
      "    accuracy                           0.47      1492\n",
      "   macro avg       0.49      0.46      0.44      1492\n",
      "weighted avg       0.49      0.47      0.44      1492\n",
      "\n",
      "[[139  29  75   0   0   0]\n",
      " [ 11 115  87   5   2   9]\n",
      " [ 32  15 182   7   0   3]\n",
      " [  0   1   2 233  23  30]\n",
      " [  0   5   4 111  36  98]\n",
      " [  0   5   5 224   3   1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "df1=pd.read_csv(\"HAR_train.csv\",header=None)\n",
    "X_train = df1.drop(df1.columns[[561]],axis=1)\n",
    "y_train= df1[561]\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"HAR.csv\", header=None)\n",
    "X_test = df.drop(df.columns[[561]],axis=1)\n",
    "y_test= df[561]\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "#Uisng Dimensionality Reduction--PCA \n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_train)\n",
    "trans=pca.transform(X_train)\n",
    "#print(\"Transformed X_train\")\n",
    "#print(trans)\n",
    "\n",
    "\n",
    "pca1 = PCA(n_components=5)\n",
    "pca1.fit(X_test)\n",
    "trans1=pca1.transform(X_test)\n",
    "#print(\"Transformed X_test\")\n",
    "#print(trans1)\n",
    "end = time.time()\n",
    "print(\"Execution time for transforming : \",end - start)\n",
    "\n",
    "\n",
    "#using Logistic \n",
    "start=time.time()\n",
    "model_logistic = LogisticRegression(random_state=10)\n",
    "model_logistic.fit(trans,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-logistic regression: \",end - start)\n",
    "y_pred = model_logistic.predict(trans1)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#using algorithms-bagging classifiers\n",
    "start=time.time()\n",
    "model = BaggingClassifier(n_estimators=100, random_state=0)\n",
    "model.fit(trans,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-bagging classifier: \",end - start)\n",
    "y_pred = model.predict(trans1)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#uisng Random forest\n",
    "\n",
    "start=time.time()\n",
    "model1 = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "model1.fit(trans,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-Random forest: \",end - start)\n",
    "y_pred = model1.predict(trans1)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#using Decision tree\n",
    "start=time.time()\n",
    "model2 = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "model2.fit(trans,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-decision tree: \",end - start)\n",
    "y_pred = model2.predict(trans1)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------dimensionality reduction-------------------------------LinearDiscriminantAnalysis-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformation-Linear :  3.6166629791259766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karthikeyan\\Anaconda3\\anoconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time-logistic regression:  0.5497117042541504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      1.00       243\n",
      "           2       0.00      0.00      0.00       229\n",
      "           3       0.09      0.10      0.09       239\n",
      "           4       0.38      0.08      0.13       289\n",
      "           5       0.03      0.03      0.03       254\n",
      "           6       0.00      0.00      0.00       238\n",
      "\n",
      "    accuracy                           0.20      1492\n",
      "   macro avg       0.25      0.20      0.21      1492\n",
      "weighted avg       0.26      0.20      0.21      1492\n",
      "\n",
      "Execution time-bagging classifier:  3.1689789295196533\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      0.99       243\n",
      "           2       0.07      0.07      0.07       229\n",
      "           3       0.06      0.05      0.06       239\n",
      "           4       0.44      0.25      0.32       289\n",
      "           5       0.01      0.00      0.00       254\n",
      "           6       0.00      0.00      0.00       238\n",
      "\n",
      "    accuracy                           0.23      1492\n",
      "   macro avg       0.26      0.23      0.24      1492\n",
      "weighted avg       0.27      0.23      0.24      1492\n",
      "\n",
      "Execution time-Random forest:  0.27135252952575684\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      0.99       243\n",
      "           2       0.01      0.01      0.01       229\n",
      "           3       0.06      0.06      0.06       239\n",
      "           4       0.44      0.30      0.36       289\n",
      "           5       0.03      0.02      0.02       254\n",
      "           6       0.00      0.00      0.00       238\n",
      "\n",
      "    accuracy                           0.23      1492\n",
      "   macro avg       0.25      0.23      0.24      1492\n",
      "weighted avg       0.26      0.23      0.25      1492\n",
      "\n",
      "Execution time-decision tree:  0.04397273063659668\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      0.99       243\n",
      "           2       0.06      0.08      0.07       229\n",
      "           3       0.05      0.05      0.05       239\n",
      "           4       0.47      0.42      0.44       289\n",
      "           5       0.04      0.02      0.02       254\n",
      "           6       0.00      0.00      0.00       238\n",
      "\n",
      "    accuracy                           0.26      1492\n",
      "   macro avg       0.27      0.26      0.26      1492\n",
      "weighted avg       0.28      0.26      0.27      1492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "df1=pd.read_csv(\"HAR_train.csv\",header=None)\n",
    "X_train = df1.drop(df1.columns[[561]],axis=1)\n",
    "y_train= df1[561]\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"HAR.csv\", header=None)\n",
    "X_test = df.drop(df.columns[[561]],axis=1)\n",
    "y_test= df[561]\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "start=time.time()\n",
    "#Uisng Dimensionality Reduction--Linear method \n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_lda = lda.fit_transform(X_test,y_test)\n",
    "#print(X_lda.shape)\n",
    "#print(X_lda)\n",
    "\n",
    "lda1 = LinearDiscriminantAnalysis()\n",
    "X1_lda = lda1.fit_transform(X_train,y_train)\n",
    "#print(X1_lda.shape)\n",
    "#print(X1_lda)\n",
    "end = time.time()\n",
    "print(\"transformation-Linear : \",end - start)\n",
    "\n",
    "#using Logistic \n",
    "start=time.time()\n",
    "model_logistic = LogisticRegression(random_state=10)\n",
    "model_logistic.fit(X1_lda,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-logistic regression: \",end - start)\n",
    "y_pred = model_logistic.predict(X_lda)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(X_lda,y_pred))\n",
    "\n",
    "\n",
    "#using algorithms-bagging classifiers\n",
    "start=time.time()\n",
    "model = BaggingClassifier(n_estimators=100, random_state=0)\n",
    "model.fit(X1_lda,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-bagging classifier: \",end - start)\n",
    "y_pred = model.predict(X_lda)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#uisng Random forest\n",
    "\n",
    "start=time.time()\n",
    "model1 = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "model1.fit(X1_lda,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-Random forest: \",end - start)\n",
    "y_pred = model1.predict(X_lda)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#using Decision tree\n",
    "start=time.time()\n",
    "model2 = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "model2.fit(X1_lda,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-decision tree: \",end - start)\n",
    "y_pred = model2.predict(X_lda)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------------------dimenensionality reduction-----------------------Random Projection Sparse---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformation-random :  0.022986650466918945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karthikeyan\\Anaconda3\\anoconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Karthikeyan\\Anaconda3\\anoconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time-logistic regression:  0.25885438919067383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.14      0.71      0.24       243\n",
      "           2       0.09      0.01      0.02       229\n",
      "           3       0.43      0.27      0.33       239\n",
      "           4       0.00      0.00      0.00       289\n",
      "           5       0.00      0.00      0.00       254\n",
      "           6       0.29      0.12      0.17       238\n",
      "\n",
      "    accuracy                           0.18      1492\n",
      "   macro avg       0.16      0.19      0.13      1492\n",
      "weighted avg       0.15      0.18      0.12      1492\n",
      "\n",
      "Execution time-bagging classifier:  1.4007163047790527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.19      0.67      0.30       243\n",
      "           2       0.19      0.15      0.17       229\n",
      "           3       0.14      0.21      0.16       239\n",
      "           4       0.35      0.02      0.04       289\n",
      "           5       0.35      0.10      0.15       254\n",
      "           6       0.10      0.01      0.02       238\n",
      "\n",
      "    accuracy                           0.19      1492\n",
      "   macro avg       0.22      0.19      0.14      1492\n",
      "weighted avg       0.23      0.19      0.14      1492\n",
      "\n",
      "Execution time-Random forest:  0.11593389511108398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.13      0.56      0.22       243\n",
      "           2       0.19      0.19      0.19       229\n",
      "           3       0.13      0.08      0.10       239\n",
      "           4       0.50      0.02      0.03       289\n",
      "           5       0.36      0.09      0.15       254\n",
      "           6       0.06      0.00      0.01       238\n",
      "\n",
      "    accuracy                           0.15      1492\n",
      "   macro avg       0.23      0.16      0.12      1492\n",
      "weighted avg       0.24      0.15      0.11      1492\n",
      "\n",
      "Execution time-decision tree:  0.0069959163665771484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.15      0.09      0.11       243\n",
      "           2       0.00      0.00      0.00       229\n",
      "           3       0.18      0.90      0.30       239\n",
      "           4       0.14      0.03      0.06       289\n",
      "           5       0.41      0.12      0.19       254\n",
      "           6       0.00      0.00      0.00       238\n",
      "\n",
      "    accuracy                           0.19      1492\n",
      "   macro avg       0.15      0.19      0.11      1492\n",
      "weighted avg       0.15      0.19      0.11      1492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"HAR.csv\", header=None)\n",
    "#print(df)\n",
    "\n",
    "#statistic report\n",
    "#print(df.describe())\n",
    "\n",
    "#training and testing\n",
    "X_test = df.drop(df.columns[[561]],axis=1)\n",
    "y_test= df[561]\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "df1=pd.read_csv(\"HAR_train.csv\",header=None)\n",
    "X_train = df.drop(df.columns[[561]],axis=1)\n",
    "y_train= df[561]\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "#Uisng Dimensionality Reduction--Randome method \n",
    "\n",
    "\n",
    "start=time.time()\n",
    "transformer = SparseRandomProjection(n_components=4)\n",
    "X_new = transformer.fit_transform(X_test)\n",
    "X_new.shape\n",
    "#print(X_new)\n",
    "\n",
    "transformer = SparseRandomProjection(n_components=4)\n",
    "X1_new = transformer.fit_transform(X_train)\n",
    "X1_new.shape\n",
    "#print(X1_new)\n",
    "end = time.time()\n",
    "print(\"transformation-random : \",end - start)\n",
    "\n",
    "#using Logistic \n",
    "start=time.time()\n",
    "model_logistic = LogisticRegression(random_state=10)\n",
    "model_logistic.fit(X1_new,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-logistic regression: \",end - start)\n",
    "y_pred = model_logistic.predict(X_new)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#using algorithms-bagging classifiers\n",
    "start=time.time()\n",
    "model = BaggingClassifier(n_estimators=100, random_state=0)\n",
    "model.fit(X1_new,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-bagging classifier: \",end - start)\n",
    "y_pred = model.predict(X_new)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#uisng Random forest\n",
    "\n",
    "start=time.time()\n",
    "model1 = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "model1.fit(X1_new,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-Random forest: \",end - start)\n",
    "y_pred = model1.predict(X_new)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#using Decision tree\n",
    "start=time.time()\n",
    "model2 = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "model2.fit(X1_new,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-decision tree: \",end - start)\n",
    "y_pred = model2.predict(X_new)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------------Dimensionality reduction--------------------Feature Agglomeration-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformation-feature :  0.8309354782104492\n",
      "Execution time-logistic regression:  0.12891101837158203\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.64      0.60       243\n",
      "           2       0.43      0.54      0.47       229\n",
      "           3       0.58      0.39      0.47       239\n",
      "           4       0.40      0.69      0.51       289\n",
      "           5       0.44      0.43      0.43       254\n",
      "           6       0.07      0.00      0.01       238\n",
      "\n",
      "    accuracy                           0.46      1492\n",
      "   macro avg       0.41      0.45      0.41      1492\n",
      "weighted avg       0.41      0.46      0.42      1492\n",
      "\n",
      "[[156  59  28   0   0   0]\n",
      " [ 65 123  32   1   8   0]\n",
      " [ 59  84  94   0   2   0]\n",
      " [  0   4   1 200  76   8]\n",
      " [  0  11   2 127 108   6]\n",
      " [  0   8   6 170  53   1]]\n",
      "Execution time-bagging classifier:  0.8430752754211426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       243\n",
      "           2       1.00      1.00      1.00       229\n",
      "           3       1.00      1.00      1.00       239\n",
      "           4       1.00      1.00      1.00       289\n",
      "           5       1.00      1.00      1.00       254\n",
      "           6       1.00      1.00      1.00       238\n",
      "\n",
      "    accuracy                           1.00      1492\n",
      "   macro avg       1.00      1.00      1.00      1492\n",
      "weighted avg       1.00      1.00      1.00      1492\n",
      "\n",
      "Execution time-Random forest:  0.06895875930786133\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.99      0.98       243\n",
      "           2       0.98      0.99      0.98       229\n",
      "           3       0.98      0.96      0.97       239\n",
      "           4       0.98      0.99      0.98       289\n",
      "           5       0.98      0.98      0.98       254\n",
      "           6       0.99      0.98      0.99       238\n",
      "\n",
      "    accuracy                           0.98      1492\n",
      "   macro avg       0.98      0.98      0.98      1492\n",
      "weighted avg       0.98      0.98      0.98      1492\n",
      "\n",
      "Execution time-decision tree:  0.008991003036499023\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.68      0.65       243\n",
      "           2       0.74      0.59      0.65       229\n",
      "           3       0.58      0.62      0.60       239\n",
      "           4       0.74      0.33      0.45       289\n",
      "           5       0.54      0.63      0.58       254\n",
      "           6       0.53      0.80      0.64       238\n",
      "\n",
      "    accuracy                           0.60      1492\n",
      "   macro avg       0.62      0.61      0.60      1492\n",
      "weighted avg       0.63      0.60      0.59      1492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets, cluster\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"HAR.csv\", header=None)\n",
    "#print(df)\n",
    "\n",
    "#statistic report\n",
    "#print(df.describe())\n",
    "\n",
    "#training and testing\n",
    "X_test = df.drop(df.columns[[561]],axis=1)\n",
    "y_test= df[561]\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "df1=pd.read_csv(\"HAR_train.csv\",header=None)\n",
    "X_train = df.drop(df.columns[[561]],axis=1)\n",
    "y_train= df[561]\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "#Uisng Dimensionality Reduction--Randome method \n",
    "\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "agglo = cluster.FeatureAgglomeration(n_clusters=4)\n",
    "agglo.fit(X_test)\n",
    "X_reduced = agglo.transform(X_test)\n",
    "X_reduced.shape\n",
    "#print(\"The X_test values\")\n",
    "#print(X_test)\n",
    "#print(\"reduced X_test\\n\\n\")\n",
    "#print(X_reduced)\n",
    "\n",
    "\n",
    "agglo = cluster.FeatureAgglomeration(n_clusters=4)\n",
    "agglo.fit(X_train)\n",
    "X1_reduced = agglo.transform(X_train)\n",
    "X1_reduced.shape\n",
    "#print(\"The X_train values\")\n",
    "#print(X_train)\n",
    "#print(\"reduced X_train\\n\\n\")\n",
    "#print(X_reduced)\n",
    "\n",
    "end = time.time()\n",
    "print(\"transformation-feature : \",end - start)\n",
    "\n",
    "#using Logistic \n",
    "start=time.time()\n",
    "model_logistic = LogisticRegression(random_state=10)\n",
    "model_logistic.fit(X1_reduced,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-logistic regression: \",end - start)\n",
    "y_pred = model_logistic.predict(X_reduced)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "\n",
    "#using algorithms-bagging classifiers\n",
    "start=time.time()\n",
    "model = BaggingClassifier(n_estimators=100, random_state=0)\n",
    "model.fit(X1_reduced,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-bagging classifier: \",end - start)\n",
    "y_pred = model.predict(X_reduced)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#uisng Random forest\n",
    "\n",
    "start=time.time()\n",
    "model1 = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "model1.fit(X1_reduced,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-Random forest: \",end - start)\n",
    "y_pred = model1.predict(X_reduced)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#using Decision tree\n",
    "start=time.time()\n",
    "model2 = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "model2.fit(X1_reduced,y_train)\n",
    "end = time.time()\n",
    "print(\"Execution time-decision tree: \",end - start)\n",
    "y_pred = model2.predict(X_reduced)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print (confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts :\n",
      " 0      1492\n",
      "1      1492\n",
      "2      1492\n",
      "3      1492\n",
      "4      1492\n",
      "       ... \n",
      "557    1492\n",
      "558    1492\n",
      "559    1492\n",
      "560    1492\n",
      "561    1492\n",
      "Length: 562, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1492 entries, 0 to 1491\n",
      "Columns: 562 entries, 0 to 561\n",
      "dtypes: float64(561), int64(1)\n",
      "memory usage: 6.4 MB\n",
      "missing values :\n",
      " None\n",
      "Sum of Row:\n",
      " 0       480.724276\n",
      "1       -19.822877\n",
      "2      -147.177858\n",
      "3      -993.050543\n",
      "4      -854.619112\n",
      "          ...     \n",
      "557      -1.073291\n",
      "558    -707.048012\n",
      "559      59.108287\n",
      "560     -85.321889\n",
      "561    5272.000000\n",
      "Length: 562, dtype: float64\n",
      "Sum of Column:\n",
      " 0      -198.222621\n",
      "1      -277.241609\n",
      "2      -289.974706\n",
      "3      -330.781823\n",
      "4      -333.507526\n",
      "           ...    \n",
      "1487   -354.816575\n",
      "1488   -353.340635\n",
      "1489   -354.119443\n",
      "1490   -355.720368\n",
      "1491   -354.935030\n",
      "Length: 1492, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"HAR.csv\", header=None)\n",
    "print(\"counts :\\n\",df.count())\n",
    "print(\"missing values :\\n\",df.info())\n",
    "print(\"Sum of Row:\\n\",df.sum(axis = 0, skipna = True))\n",
    "print(\"Sum of Column:\\n\",df.sum(axis = 1, skipna = True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
